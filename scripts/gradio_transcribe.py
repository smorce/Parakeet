import nemo.collections.asr as nemo_asr
import sys, torch
import os
import librosa
import soundfile as sf
import tempfile
import gc
import math
import gradio as gr
import speech_recognition as sr
# æ³¨æ„: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿèƒ½ã¯ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–ã®JavaScriptã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€
# Pythonå´ã§ã®éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ä¸è¦ã«ãªã‚Šã¾ã—ãŸ

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒã‚¤ã‚¹çŠ¶æ…‹ã‚’ç®¡ç†
model = None
model_device = None
model_loaded = False

# GPU/CPUè‡ªå‹•åˆ¤å®šã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
device = "cpu"
if torch.cuda.is_available():
    try:
        # CUDAåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        torch.cuda.current_device()
        device = "cuda"
        print(f"[INFO] CUDA GPU detected: {torch.cuda.get_device_name()}")
        # GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’è¡¨ç¤º
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"[INFO] Total GPU memory: {total_memory:.2f} GB")
    except RuntimeError as e:
        print(f"[WARNING] CUDA available but initialization failed: {e}")
        print("[INFO] Falling back to CPU")
        device = "cpu"
else:
    print("[INFO] CUDA not available, using CPU")

# CPUã‚’å¼·åˆ¶ã™ã‚‹ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
if os.environ.get("FORCE_CPU", "").lower() in ("1", "true", "yes"):
    device = "cpu"
    print("[INFO] FORCE_CPU environment variable set, using CPU")

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
if device == "cpu":
    torch.set_default_device("cpu")

# æ–°ã—ã„ãƒã‚¤ã‚¯éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹é–¢æ•°
def transcribe_mic_audio(audio_file, progress=gr.Progress()):
    """ãƒ–ãƒ©ã‚¦ã‚¶ã§éŒ²éŸ³ã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆWAV/OGGãªã©ï¼‰ã‚’æ–‡å­—èµ·ã“ã—"""
    if audio_file is None:
        return "éŒ²éŸ³ã•ã‚ŒãŸéŸ³å£°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"

    try:
        recognizer = sr.Recognizer()
        progress(0.2, desc="éŸ³å£°ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        with sr.AudioFile(audio_file) as source:
            audio_data = recognizer.record(source)
        progress(0.6, desc="éŸ³å£°ã‚’èªè­˜ä¸­...")
        text = recognizer.recognize_google(audio_data, language='ja-JP')
        progress(1.0, desc="èªè­˜å®Œäº†")
        return text
    except sr.UnknownValueError:
        return "éŸ³å£°ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    except sr.RequestError as e:
        return f"Google Web Speech APIã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}"
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# æ³¨æ„: ä»¥å‰ã®Pythonãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿèƒ½ã¯ã€
# ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–ã®JavaScriptå®Ÿè£…ã«ç½®ãæ›ãˆã‚‰ã‚Œã¾ã—ãŸ

def load_model_with_fallback(device_preference="cuda", progress=gr.Progress()):
    """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã¯CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    global model, model_device, model_loaded
    
    if model_loaded and model is not None:
        return model, model_device
    
    try:
        progress(0.1, desc=f"{device_preference.upper()}ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        print(f"[INFO] Attempting to load model on {device_preference.upper()}")
        
        progress(0.3, desc="Parakeet-TDT-CTCãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        asr = nemo_asr.models.ASRModel.from_pretrained(
            model_name="nvidia/parakeet-tdt_ctc-0.6b-ja"
        )
        
        if device_preference == "cuda" and torch.cuda.is_available():
            progress(0.6, desc="ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ä¸­...")
            # GPUä½¿ç”¨å‰ã«ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
            torch.cuda.empty_cache()
            gc.collect()
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•
            asr = asr.cuda()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"[INFO] GPU memory after model loading - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB")
            
            model = asr
            model_device = "cuda"
            model_loaded = True
            progress(1.0, desc="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆGPUï¼‰")
            return asr, "cuda"
        else:
            progress(0.8, desc="ãƒ¢ãƒ‡ãƒ«ã‚’CPUã«è¨­å®šä¸­...")
            asr = asr.cpu()
            model = asr
            model_device = "cpu"
            model_loaded = True
            progress(1.0, desc="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆCPUï¼‰")
            return asr, "cpu"
            
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print(f"[WARNING] GPU out of memory during model loading: {e}")
            print("[INFO] Falling back to CPU")
            progress(0.7, desc="GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­...")
            
            # GPU ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
            
            # CPUã§ãƒ¢ãƒ‡ãƒ«ã‚’å†èª­ã¿è¾¼ã¿
            asr = nemo_asr.models.ASRModel.from_pretrained(
                model_name="nvidia/parakeet-tdt_ctc-0.6b-ja"
            )
            asr = asr.cpu()
            model = asr
            model_device = "cpu"
            model_loaded = True
            progress(1.0, desc="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆCPUï¼‰")
            return asr, "cpu"
        else:
            progress(1.0, desc=f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise e

def split_audio_for_memory_efficiency(audio_path, max_duration=30):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²"""
    y, sr = librosa.load(audio_path, sr=16000, mono=True)
    duration = len(y) / sr
    
    if duration <= max_duration:
        return [audio_path]
    
    # åˆ†å‰²æ•°ã‚’è¨ˆç®—
    num_splits = math.ceil(duration / max_duration)
    chunk_length = len(y) // num_splits
    
    temp_files = []
    for i in range(num_splits):
        start_idx = i * chunk_length
        end_idx = min((i + 1) * chunk_length, len(y))
        chunk = y[start_idx:end_idx]
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with tempfile.NamedTemporaryFile(suffix=f"_chunk_{i}.wav", delete=False) as tmpfile:
            sf.write(tmpfile.name, chunk, sr)
            temp_files.append(tmpfile.name)
    
    return temp_files

def transcribe_with_memory_management(asr, audio_path, device, max_duration=30, progress=gr.Progress()):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªéŸ³å£°è»¢å†™ï¼ˆéŸ³å£°åˆ†å‰²å¯¾å¿œï¼‰"""
    try:
        progress(0.1, desc="éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æä¸­...")
        
        # é•·ã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¯åˆ†å‰²
        audio_chunks = split_audio_for_memory_efficiency(audio_path, max_duration)
        all_transcriptions = []
        
        total_chunks = len(audio_chunks)
        print(f"[INFO] Processing {total_chunks} audio chunks")
        
        for i, chunk_path in enumerate(audio_chunks):
            try:
                # é€²æ—æ›´æ–°
                chunk_progress = 0.2 + (0.7 * i / total_chunks)
                progress(chunk_progress, desc=f"éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ä¸­... ({i+1}/{total_chunks})")
                
                # GPUä½¿ç”¨æ™‚ã¯ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                if device == "cuda":
                    torch.cuda.empty_cache()
                    gc.collect()
                
                # è»¢å†™å®Ÿè¡Œ
                results = asr.transcribe([chunk_path])
                
                # çµæœã‚’å–å¾—ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                for r in results:
                    all_transcriptions.append(r.text)
                
                # GPUä½¿ç”¨æ™‚ã¯å†åº¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                if device == "cuda":
                    torch.cuda.empty_cache()
                    gc.collect()
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower() and device == "cuda":
                    print(f"[WARNING] GPU out of memory during chunk transcription: {e}")
                    print("[INFO] Falling back to CPU for this chunk")
                    
                    progress(chunk_progress, desc=f"GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­... ({i+1}/{total_chunks})")
                    
                    # GPU ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                    torch.cuda.empty_cache()
                    gc.collect()
                    
                    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€æ™‚çš„ã«CPUã«ç§»å‹•
                    asr_cpu = asr.cpu()
                    results = asr_cpu.transcribe([chunk_path])
                    for r in results:
                        all_transcriptions.append(r.text)
                    
                    # ãƒ¢ãƒ‡ãƒ«ã‚’å…ƒã®ãƒ‡ãƒã‚¤ã‚¹ã«æˆ»ã™ï¼ˆæ¬¡å›ã®ãŸã‚ã«ï¼‰
                    if device == "cuda":
                        try:
                            asr.cuda()
                        except RuntimeError:
                            print("[WARNING] Cannot move model back to GPU, staying on CPU")
                            device = "cpu"
                else:
                    raise e
            
            finally:
                # åˆ†å‰²ã—ãŸä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if chunk_path != audio_path and os.path.exists(chunk_path):
                    os.remove(chunk_path)
        
        progress(0.95, desc="çµæœã‚’ã¾ã¨ã‚ä¸­...")
        return all_transcriptions
        
    except Exception as e:
        # åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for chunk_path in audio_chunks if 'audio_chunks' in locals() else []:
            if chunk_path != audio_path and os.path.exists(chunk_path):
                os.remove(chunk_path)
        raise e

def process_audio_file(audio_file, progress=gr.Progress()):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œ"""
    if audio_file is None:
        return "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯èª­ã¿è¾¼ã‚€
        global model, model_device
        model_was_loaded = model_loaded
        if not model_loaded:
            progress(0.0, desc="ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            model, model_device = load_model_with_fallback(device, progress)
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        audio_path = audio_file
        if hasattr(audio_file, 'name'):
            audio_path = audio_file.name
        
        print(f"[INFO] Processing: {audio_path}")
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªè»¢å†™ï¼ˆéŸ³å£°åˆ†å‰²å¯¾å¿œï¼‰
        transcriptions = transcribe_with_memory_management(model, audio_path, model_device, progress=progress)
        
        # çµæœã‚’çµåˆ
        final_result = "\n".join(transcriptions)
        
        progress(1.0, desc="æ–‡å­—èµ·ã“ã—å®Œäº†ï¼")
        
        return final_result
            
    except Exception as e:
        error_msg = f"æ–‡å­—èµ·ã“ã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"
        print(f"[ERROR] {error_msg}")
        return error_msg
    finally:
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        gc.collect()
        if model_device == "cuda":
            torch.cuda.empty_cache()

# Gradio ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä½œæˆ
def get_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
    info = {
        "device": device.upper(),
        "cuda_available": torch.cuda.is_available(),
        "gpu_count": 0,
        "gpu_name": "None",
        "total_memory": "N/A",
        "allocated_memory": "N/A",
        "cached_memory": "N/A"
    }
    
    if torch.cuda.is_available():
        info["gpu_count"] = torch.cuda.device_count()
        info["gpu_name"] = torch.cuda.get_device_name(0)
        
        try:
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            info["total_memory"] = f"{total_memory:.2f} GB"
            
            if model_loaded:
                allocated = torch.cuda.memory_allocated() / 1024**3
                cached = torch.cuda.memory_reserved() / 1024**3
                info["allocated_memory"] = f"{allocated:.2f} GB"
                info["cached_memory"] = f"{cached:.2f} GB"
            else:
                info["allocated_memory"] = "ãƒ¢ãƒ‡ãƒ«æœªèª­ã¿è¾¼ã¿"
                info["cached_memory"] = "ãƒ¢ãƒ‡ãƒ«æœªèª­ã¿è¾¼ã¿"
        except:
            pass
    
    return info

def create_gradio_interface():
    """Gradio ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ"""
    
    # ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ç”¨ã®JavaScript
    realtime_js = """
    <script>
    // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã®çŠ¶æ…‹ç®¡ç†
    let realtimeState = {
        recognition: null,
        stream: null,
        isActive: false,
        finalTranscript: '',
        results: []
    };

    // Web Speech API ã‚µãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
    function checkWebSpeechSupport() {
        return 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window;
    }

    // ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°é–¢æ•°
    function updateStatus(message) {
        const statusElement = document.getElementById('realtime_status');
        if (statusElement && statusElement.querySelector('textarea')) {
            statusElement.querySelector('textarea').value = message;
        }
    }

    // çµæœè¡¨ç¤ºæ›´æ–°é–¢æ•°
    function updateResults(transcript, isFinal) {
        const outputElement = document.getElementById('realtime_output');
        if (outputElement && outputElement.querySelector('textarea')) {
            const textarea = outputElement.querySelector('textarea');
            const timestamp = new Date().toLocaleTimeString('ja-JP');
            
            if (isFinal) {
                realtimeState.finalTranscript += transcript + '\\n';
                textarea.value = realtimeState.finalTranscript;
            } else {
                textarea.value = realtimeState.finalTranscript + transcript;
            }
            
            // è‡ªå‹•ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
            textarea.scrollTop = textarea.scrollHeight;
            
            // æœ€çµ‚çµæœã®ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            if (isFinal) {
                realtimeState.results.push(`[${timestamp}] ${transcript}`);
            }
        }
    }
    
    // Web Speech Recognitionè¨­å®š
    function setupSpeechRecognition() {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        
        if (!SpeechRecognition) {
            updateStatus('âŒ Web Speech APIãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“');
            return false;
        }
        
        realtimeState.recognition = new SpeechRecognition();
        realtimeState.recognition.lang = 'ja-JP';
        realtimeState.recognition.interimResults = true;
        realtimeState.recognition.continuous = true;
        
        realtimeState.recognition.onstart = function() {
            updateStatus('ğŸ¤ éŸ³å£°èªè­˜ä¸­... è©±ã—ã‹ã‘ã¦ãã ã•ã„');
        };
        
        realtimeState.recognition.onresult = function(event) {
            let interimTranscript = '';
            for (let i = event.resultIndex; i < event.results.length; ++i) {
                if (event.results[i].isFinal) {
                    updateResults(event.results[i][0].transcript, true);
                } else {
                    interimTranscript += event.results[i][0].transcript;
                }
            }
            if (interimTranscript) {
                updateResults(interimTranscript, false);
            }
        };
        
        realtimeState.recognition.onerror = function(event) {
            // 'aborted' ã¯ stopRealtimeRecognition() ã«ã‚ˆã‚‹æ­£å¸¸çµ‚äº†ãªã®ã§ç„¡è¦–ã™ã‚‹
            if (event.error === 'aborted') {
                updateStatus('â¹ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜ã‚’åœæ­¢ã—ã¾ã—ãŸ');
                return;
            }
            console.error('éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼:', event.error);
            updateStatus(`âŒ ã‚¨ãƒ©ãƒ¼: ${event.error}`);
        };
        
        realtimeState.recognition.onend = function() {
            if (realtimeState.isActive) {
                realtimeState.recognition.start(); // é€£ç¶šèªè­˜
            } else {
                updateStatus('â¹ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜ã‚’åœæ­¢ã—ã¾ã—ãŸ');
            }
        };
        
        return true;
    }

    // ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã«é–¢æ•°ã‚’é…ç½®
    window.startRealtimeRecognition = async function() {
        try {
            if (realtimeState.isActive) {
                console.warn("Recognition already active");
                return;
            }
            if (!checkWebSpeechSupport()) {
                updateStatus('âŒ Web Speech APIãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“');
                return;
            }
            
            updateStatus('ğŸ¤ ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¦æ±‚ä¸­...');
            
            realtimeState.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            if (!setupSpeechRecognition()) {
                return;
            }
            
            realtimeState.isActive = true;
            realtimeState.recognition.start();
            
        } catch (error) {
            console.error('ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜é–‹å§‹ã‚¨ãƒ©ãƒ¼:', error);
            updateStatus(`âŒ ã‚¨ãƒ©ãƒ¼: ${error.message}`);
        }
    }

    window.stopRealtimeRecognition = function() {
        realtimeState.isActive = false;
        if (realtimeState.recognition) {
            // abort() ã¯ 'aborted' ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºç«ã™ã‚‹ãŸã‚ã€stop() ã‚’ä½¿ç”¨ã—ã¦ç©ã‚„ã‹ã«çµ‚äº†
            realtimeState.recognition.stop();
        }
        if (realtimeState.stream) {
            realtimeState.stream.getTracks().forEach(track => track.stop());
        }
    }

    window.clearResults = function() {
        const outputElement = document.getElementById('realtime_output');
        if (outputElement && outputElement.querySelector('textarea')) {
            outputElement.querySelector('textarea').value = '';
        }
        realtimeState.results = [];
        realtimeState.finalTranscript = '';
        updateStatus('ğŸ—‘ï¸ çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ');
    }

    window.downloadResults = function() {
        if (realtimeState.results.length === 0) {
            alert('ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚');
            return;
        }
        
        const text = realtimeState.results.join('\\n');
        const blob = new Blob([text], { type: 'text/plain;charset=utf-8' });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `realtime_transcription_${new Date().toISOString().slice(0,19).replace(/:/g,'-')}.txt`;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        URL.revokeObjectURL(url);
    }

    // ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰å¾Œã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ç™»éŒ²ï¼ˆGradioå†ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¯¾ç­–ï¼‰
    window.addEventListener('load', () => {
        globalThis.startRealtimeRecognition = window.startRealtimeRecognition;
        globalThis.stopRealtimeRecognition  = window.stopRealtimeRecognition;
        globalThis.clearResults            = window.clearResults;
        globalThis.downloadResults         = window.downloadResults;
    });
    </script>
    """
    
    with gr.Blocks(title="éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚·ã‚¹ãƒ†ãƒ ", theme=gr.themes.Soft(), head=realtime_js) as interface:
        gr.Markdown("""
        # ğŸ¤ éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚·ã‚¹ãƒ†ãƒ 
        
        NVIDIA Parakeet-TDT-CTCãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã¾ãŸã¯Google Web Speech APIï¼ˆãƒã‚¤ã‚¯ï¼‰ã‚’ä½¿ç”¨ã—ãŸæ—¥æœ¬èªéŸ³å£°æ–‡å­—èµ·ã“ã—ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        """)
        
        with gr.Tabs():
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¿ãƒ–
            with gr.TabItem("â¬†ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—"):
                with gr.Row():
                    with gr.Column(scale=1):
                        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                        audio_input = gr.Audio(
                            label="éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«",
                            type="filepath",
                            sources=["upload"]
                        )
                        
                        # å®Ÿè¡Œãƒœã‚¿ãƒ³
                        transcribe_btn = gr.Button(
                            "ğŸš€ æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ",
                            variant="primary",
                            size="lg"
                        )
                    
                    with gr.Column(scale=2):
                        # çµæœè¡¨ç¤º
                        output_text = gr.Textbox(
                            label="æ–‡å­—èµ·ã“ã—çµæœ",
                            lines=15,
                            max_lines=30,
                            placeholder="ã“ã“ã«æ–‡å­—èµ·ã“ã—çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™...",
                            show_copy_button=True
                        )
                
                # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤ºï¼ˆå‹•çš„æ›´æ–°ï¼‰
                with gr.Row():
                    with gr.Column():
                        system_info_display = gr.Markdown(
                            value="### ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±\nèª­ã¿è¾¼ã¿ä¸­...",
                            label="ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±"
                        )
                        
                        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±æ›´æ–°ãƒœã‚¿ãƒ³
                        refresh_btn = gr.Button("ğŸ”„ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’æ›´æ–°", size="sm")

            # ãƒã‚¤ã‚¯å…¥åŠ›ã‚¿ãƒ–
            with gr.TabItem("ğŸ™ï¸ ãƒã‚¤ã‚¯ã‹ã‚‰æ–‡å­—èµ·ã“ã—"):
                with gr.Row():
                    with gr.Column(scale=1):
                        mic_audio = gr.Audio(
                            label="ãƒã‚¤ã‚¯éŒ²éŸ³",
                            type="filepath",
                            sources=["microphone"]
                        )
                        mic_transcribe_btn = gr.Button(
                            "ğŸ¤ æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ",
                            variant="primary",
                            size="lg"
                        )

                    with gr.Column(scale=2):
                        mic_output_text = gr.Textbox(
                            label="æ–‡å­—èµ·ã“ã—çµæœ",
                            lines=15,
                            max_lines=30,
                            placeholder="ã“ã“ã«ãƒã‚¤ã‚¯ã‹ã‚‰ã®æ–‡å­—èµ·ã“ã—çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™...",
                            show_copy_button=True
                        )

            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚¿ãƒ–
            with gr.TabItem("âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—"):
                gr.Markdown("""
                ### ğŸ¯ ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ– ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜
                
                Web Speech APIã«ã‚ˆã‚‹éŸ³å£°æ¤œå‡ºã¨MediaRecorderã«ã‚ˆã‚‹é«˜å“è³ªéŒ²éŸ³ã‚’çµ„ã¿åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ã§ã™ã€‚
                
                **ç‰¹å¾´:**
                - ğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ– Web Speech API ã«ã‚ˆã‚‹éŸ³å£°æ¤œå‡º
                - ğŸ¤ MediaRecorder ã«ã‚ˆã‚‹é«˜å“è³ªéŸ³å£°éŒ²éŸ³
                - âš¡ éŸ³å£°æ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå‹•éŒ²éŸ³é–‹å§‹ãƒ»çµ‚äº†
                - ğŸ”„ Google Cloud Speech API ã«ã‚ˆã‚‹é«˜ç²¾åº¦æ–‡å­—èµ·ã“ã—
                - ğŸ¯ ãƒãƒ³ã‚ºãƒ•ãƒªãƒ¼æ“ä½œï¼ˆãƒœã‚¿ãƒ³æ“ä½œä¸è¦ï¼‰
                
                **ä½¿ã„æ–¹:**
                1. ã€Œé–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãƒã‚¤ã‚¯æ¨©é™ã‚’è¨±å¯
                2. è©±ã—ã‹ã‘ã‚‹ã¨è‡ªå‹•çš„ã«éŒ²éŸ³ãƒ»æ–‡å­—èµ·ã“ã—ãŒé–‹å§‹ã•ã‚Œã¾ã™
                3. ç„¡éŸ³çŠ¶æ…‹ã«ãªã‚‹ã¨è‡ªå‹•çš„ã«éŒ²éŸ³åœæ­¢ãƒ»çµæœè¡¨ç¤º
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³
                        with gr.Row():
                            realtime_start_btn = gr.Button(
                                "ğŸ¤ é–‹å§‹",
                                variant="primary",
                                size="lg",
                                elem_id="realtime_start_btn"
                            )
                            realtime_stop_btn = gr.Button(
                                "â¹ï¸ åœæ­¢",
                                variant="secondary",
                                size="lg",
                                elem_id="realtime_stop_btn"
                            )
                        
                        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
                        realtime_status = gr.Textbox(
                            label="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
                            value="å¾…æ©Ÿä¸­ - ã€Œé–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„",
                            lines=3,
                            interactive=False,
                            elem_id="realtime_status"
                        )
                        
                        # è¨­å®šãƒ‘ãƒãƒ«
                        with gr.Accordion("âš™ï¸ è©³ç´°è¨­å®š", open=False):
                            gr.Markdown("""
                            **éŸ³å£°èªè­˜è¨­å®š:**
                            - è¨€èª: æ—¥æœ¬èª (ja-JP)
                            - é€£ç¶šèªè­˜: æœ‰åŠ¹
                            - ä¸­é–“çµæœ: æœ‰åŠ¹
                            
                            **éŒ²éŸ³è¨­å®š:**
                            - ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: 44.1kHz
                            - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: WebM/MP3
                            - ãƒã‚¤ã‚ºé™¤å»: æœ‰åŠ¹
                            """)
                            
                            api_key_input = gr.Textbox(
                                label="Google Cloud Speech API Key (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)",
                                placeholder="ç‹¬è‡ªã®APIã‚­ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯å…¥åŠ›ã—ã¦ãã ã•ã„",
                                type="password",
                                elem_id="api_key_input"
                            )
                    
                    with gr.Column(scale=2):
                        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµæœè¡¨ç¤º
                        realtime_output = gr.Textbox(
                            label="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—çµæœ",
                            lines=20,
                            max_lines=30,
                            placeholder="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹ã™ã‚‹ã¨ã€ã“ã“ã«çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™...\n\nè©±ã—ã‹ã‘ã¦ãã ã•ã„ï¼",
                            show_copy_button=True,
                            autoscroll=True,
                            elem_id="realtime_output"
                        )
                        
                        # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³
                        with gr.Row():
                            clear_results_btn = gr.Button(
                                "ğŸ—‘ï¸ çµæœã‚’ã‚¯ãƒªã‚¢",
                                size="sm",
                                elem_id="clear_results_btn"
                            )
                            download_results_btn = gr.Button(
                                "ğŸ’¾ çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                size="sm",
                                elem_id="download_results_btn"
                        )
        
        def update_system_info():
            """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’æ›´æ–°"""
            info = get_system_info()
            
            markdown_text = f"""
### ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±

#### ğŸ–¥ï¸ ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±
- **ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹**: {info['device']}
- **CUDAåˆ©ç”¨å¯èƒ½**: {'âœ… ã¯ã„' if info['cuda_available'] else 'âŒ ã„ã„ãˆ'}
- **GPUæ•°**: {info['gpu_count']}
- **GPUå**: {info['gpu_name']}

#### ğŸ’¾ ãƒ¡ãƒ¢ãƒªæƒ…å ±
- **ç·GPU ãƒ¡ãƒ¢ãƒª**: {info['total_memory']}
- **ä½¿ç”¨ä¸­ãƒ¡ãƒ¢ãƒª**: {info['allocated_memory']}
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒª**: {info['cached_memory']}

#### ğŸ¤– ãƒ¢ãƒ‡ãƒ«æƒ…å ±
- **ãƒ¢ãƒ‡ãƒ«**: nvidia/parakeet-tdt_ctc-0.6b-ja, silero-vad
- **ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹**: {'âœ… èª­ã¿è¾¼ã¿æ¸ˆã¿' if model_loaded else 'â³ æœªèª­ã¿è¾¼ã¿'}
- **å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: WAV, MP3, FLAC, M4A ãªã©
"""
            return markdown_text
        
        # ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–æ©Ÿèƒ½ã®ãŸã‚ã€Pythonã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã¯ä¸è¦
        # JavaScriptã§ç›´æ¥å‡¦ç†ã•ã‚Œã¾ã™
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
        transcribe_btn.click(
            fn=process_audio_file,
            inputs=[audio_input],
            outputs=[output_text],
            show_progress=True
        )

        mic_transcribe_btn.click(
            fn=transcribe_mic_audio,
            inputs=[mic_audio],
            outputs=[mic_output_text],
            show_progress=True
        )
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿèƒ½ã®JSã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¤ãƒ³ãƒ‰
        realtime_start_btn.click(
            fn=None,
            inputs=[],
            outputs=[],
            js="() => startRealtimeRecognition()"
        )
        realtime_stop_btn.click(
            fn=None,
            inputs=[],
            outputs=[],
            js="() => stopRealtimeRecognition()"
        )
        clear_results_btn.click(
            fn=None,
            inputs=[],
            outputs=[],
            js="() => clearResults()"
        )
        download_results_btn.click(
            fn=None,
            inputs=[],
            outputs=[],
            js="() => downloadResults()"
        )
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±æ›´æ–°ãƒãƒ³ãƒ‰ãƒ©
        refresh_btn.click(
            fn=update_system_info,
            outputs=[system_info_display]
        )
        
        # åˆæœŸè¡¨ç¤ºæ™‚ã«ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’æ›´æ–°
        interface.load(
            fn=update_system_info,
            outputs=[system_info_display]
        )
    
    return interface

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒã‚ã‚‹å ´åˆã¯å¾“æ¥ã®å‡¦ç†ã‚’å®Ÿè¡Œ
    if len(sys.argv) > 1:
        try:
            asr, actual_device = load_model_with_fallback(device)
            print(f"[INFO] Model loaded successfully on {actual_device.upper()}")
            
        except Exception as e:
            print(f"[ERROR] Model loading failed: {e}")
            sys.exit(1)

        audio_files = sys.argv[1:]
        if not audio_files:
            print("[ERROR] No audio files specified")
            sys.exit(1)

        for audio_file in audio_files:
            print(f"[INFO] Processing: {audio_file}")
            try:
                # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªè»¢å†™ï¼ˆéŸ³å£°åˆ†å‰²å¯¾å¿œï¼‰
                transcriptions = transcribe_with_memory_management(asr, audio_file, actual_device)
                
                for text in transcriptions:
                    print(text)
                    
            except Exception as e:
                print(f"[ERROR] Transcription failed for {audio_file}: {e}")
            finally:
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                if actual_device == "cuda":
                    torch.cuda.empty_cache()
    else:
        # Gradio UIã‚’èµ·å‹•
        print("[INFO] Starting Gradio interface...")
        interface = create_gradio_interface()
        interface.launch(
            server_name="0.0.0.0",
            server_port=3791,
            share=False,
            show_error=True
        )