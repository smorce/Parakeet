import nemo.collections.asr as nemo_asr
import sys, torch
import os
import librosa
import soundfile as sf
import tempfile
import gc
import math
import gradio as gr

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒã‚¤ã‚¹çŠ¶æ…‹ã‚’ç®¡ç†
model = None
model_device = None
model_loaded = False

# GPU/CPUè‡ªå‹•åˆ¤å®šã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
device = "cpu"
if torch.cuda.is_available():
    try:
        # CUDAåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        torch.cuda.current_device()
        device = "cuda"
        print(f"[INFO] CUDA GPU detected: {torch.cuda.get_device_name()}")
        # GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’è¡¨ç¤º
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"[INFO] Total GPU memory: {total_memory:.2f} GB")
    except RuntimeError as e:
        print(f"[WARNING] CUDA available but initialization failed: {e}")
        print("[INFO] Falling back to CPU")
        device = "cpu"
else:
    print("[INFO] CUDA not available, using CPU")

# CPUã‚’å¼·åˆ¶ã™ã‚‹ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
if os.environ.get("FORCE_CPU", "").lower() in ("1", "true", "yes"):
    device = "cpu"
    print("[INFO] FORCE_CPU environment variable set, using CPU")

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
if device == "cpu":
    torch.set_default_device("cpu")

def load_model_with_fallback(device_preference="cuda", progress=gr.Progress()):
    """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã¯CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    global model, model_device, model_loaded
    
    if model_loaded and model is not None:
        return model, model_device
    
    try:
        progress(0.1, desc=f"{device_preference.upper()}ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        print(f"[INFO] Attempting to load model on {device_preference.upper()}")
        
        progress(0.3, desc="Parakeet-TDT-CTCãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        asr = nemo_asr.models.ASRModel.from_pretrained(
            model_name="nvidia/parakeet-tdt_ctc-0.6b-ja"
        )
        
        if device_preference == "cuda" and torch.cuda.is_available():
            progress(0.6, desc="ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ä¸­...")
            # GPUä½¿ç”¨å‰ã«ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
            torch.cuda.empty_cache()
            gc.collect()
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•
            asr = asr.cuda()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"[INFO] GPU memory after model loading - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB")
            
            model = asr
            model_device = "cuda"
            model_loaded = True
            progress(1.0, desc="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆGPUï¼‰")
            return asr, "cuda"
        else:
            progress(0.8, desc="ãƒ¢ãƒ‡ãƒ«ã‚’CPUã«è¨­å®šä¸­...")
            asr = asr.cpu()
            model = asr
            model_device = "cpu"
            model_loaded = True
            progress(1.0, desc="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆCPUï¼‰")
            return asr, "cpu"
            
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print(f"[WARNING] GPU out of memory during model loading: {e}")
            print("[INFO] Falling back to CPU")
            progress(0.7, desc="GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­...")
            
            # GPU ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
            
            # CPUã§ãƒ¢ãƒ‡ãƒ«ã‚’å†èª­ã¿è¾¼ã¿
            asr = nemo_asr.models.ASRModel.from_pretrained(
                model_name="nvidia/parakeet-tdt_ctc-0.6b-ja"
            )
            asr = asr.cpu()
            model = asr
            model_device = "cpu"
            model_loaded = True
            progress(1.0, desc="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆCPUï¼‰")
            return asr, "cpu"
        else:
            progress(1.0, desc=f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise e

def split_audio_for_memory_efficiency(audio_path, max_duration=30):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²"""
    y, sr = librosa.load(audio_path, sr=16000, mono=True)
    duration = len(y) / sr
    
    if duration <= max_duration:
        return [audio_path]
    
    # åˆ†å‰²æ•°ã‚’è¨ˆç®—
    num_splits = math.ceil(duration / max_duration)
    chunk_length = len(y) // num_splits
    
    temp_files = []
    for i in range(num_splits):
        start_idx = i * chunk_length
        end_idx = min((i + 1) * chunk_length, len(y))
        chunk = y[start_idx:end_idx]
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with tempfile.NamedTemporaryFile(suffix=f"_chunk_{i}.wav", delete=False) as tmpfile:
            sf.write(tmpfile.name, chunk, sr)
            temp_files.append(tmpfile.name)
    
    return temp_files

def transcribe_with_memory_management(asr, audio_path, device, max_duration=30, progress=gr.Progress()):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªéŸ³å£°è»¢å†™ï¼ˆéŸ³å£°åˆ†å‰²å¯¾å¿œï¼‰"""
    try:
        progress(0.1, desc="éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æä¸­...")
        
        # é•·ã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¯åˆ†å‰²
        audio_chunks = split_audio_for_memory_efficiency(audio_path, max_duration)
        all_transcriptions = []
        
        total_chunks = len(audio_chunks)
        print(f"[INFO] Processing {total_chunks} audio chunks")
        
        for i, chunk_path in enumerate(audio_chunks):
            try:
                # é€²æ—æ›´æ–°
                chunk_progress = 0.2 + (0.7 * i / total_chunks)
                progress(chunk_progress, desc=f"éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ä¸­... ({i+1}/{total_chunks})")
                
                # GPUä½¿ç”¨æ™‚ã¯ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                if device == "cuda":
                    torch.cuda.empty_cache()
                    gc.collect()
                
                # è»¢å†™å®Ÿè¡Œ
                results = asr.transcribe([chunk_path])
                
                # çµæœã‚’å–å¾—ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                for r in results:
                    all_transcriptions.append(r.text)
                
                # GPUä½¿ç”¨æ™‚ã¯å†åº¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                if device == "cuda":
                    torch.cuda.empty_cache()
                    gc.collect()
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower() and device == "cuda":
                    print(f"[WARNING] GPU out of memory during chunk transcription: {e}")
                    print("[INFO] Falling back to CPU for this chunk")
                    
                    progress(chunk_progress, desc=f"GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­... ({i+1}/{total_chunks})")
                    
                    # GPU ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                    torch.cuda.empty_cache()
                    gc.collect()
                    
                    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€æ™‚çš„ã«CPUã«ç§»å‹•
                    asr_cpu = asr.cpu()
                    results = asr_cpu.transcribe([chunk_path])
                    for r in results:
                        all_transcriptions.append(r.text)
                    
                    # ãƒ¢ãƒ‡ãƒ«ã‚’å…ƒã®ãƒ‡ãƒã‚¤ã‚¹ã«æˆ»ã™ï¼ˆæ¬¡å›ã®ãŸã‚ã«ï¼‰
                    if device == "cuda":
                        try:
                            asr.cuda()
                        except RuntimeError:
                            print("[WARNING] Cannot move model back to GPU, staying on CPU")
                            device = "cpu"
                else:
                    raise e
            
            finally:
                # åˆ†å‰²ã—ãŸä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if chunk_path != audio_path and os.path.exists(chunk_path):
                    os.remove(chunk_path)
        
        progress(0.95, desc="çµæœã‚’ã¾ã¨ã‚ä¸­...")
        return all_transcriptions
        
    except Exception as e:
        # åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for chunk_path in audio_chunks if 'audio_chunks' in locals() else []:
            if chunk_path != audio_path and os.path.exists(chunk_path):
                os.remove(chunk_path)
        raise e

def process_audio_file(audio_file, progress=gr.Progress()):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œ"""
    if audio_file is None:
        return "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯èª­ã¿è¾¼ã‚€
        global model, model_device
        if not model_loaded:
            progress(0.0, desc="ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            model, model_device = load_model_with_fallback(device, progress)
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        audio_path = audio_file
        if hasattr(audio_file, 'name'):
            audio_path = audio_file.name
        
        print(f"[INFO] Processing: {audio_path}")
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªè»¢å†™ï¼ˆéŸ³å£°åˆ†å‰²å¯¾å¿œï¼‰
        transcriptions = transcribe_with_memory_management(model, audio_path, model_device, progress=progress)
        
        # çµæœã‚’çµåˆ
        final_result = "\n".join(transcriptions)
        
        progress(1.0, desc="æ–‡å­—èµ·ã“ã—å®Œäº†ï¼")
        
        return final_result
            
    except Exception as e:
        error_msg = f"æ–‡å­—èµ·ã“ã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"
        print(f"[ERROR] {error_msg}")
        return error_msg
    finally:
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        gc.collect()
        if model_device == "cuda":
            torch.cuda.empty_cache()

# Gradio ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä½œæˆ
def create_gradio_interface():
    """Gradio ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ"""
    
    with gr.Blocks(title="éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚·ã‚¹ãƒ†ãƒ ", theme=gr.themes.Soft()) as interface:
        gr.Markdown("""
        # ğŸ¤ éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚·ã‚¹ãƒ†ãƒ 
        
        NVIDIA Parakeet-TDT-CTCãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ—¥æœ¬èªéŸ³å£°æ–‡å­—èµ·ã“ã—ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€Œæ–‡å­—èµ·ã“ã—å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                audio_input = gr.Audio(
                    label="éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«",
                    type="filepath",
                    sources=["upload"]
                )
                
                # å®Ÿè¡Œãƒœã‚¿ãƒ³
                transcribe_btn = gr.Button(
                    "ğŸš€ æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ",
                    variant="primary",
                    size="lg"
                )
            
            with gr.Column(scale=2):
                # çµæœè¡¨ç¤º
                output_text = gr.Textbox(
                    label="æ–‡å­—èµ·ã“ã—çµæœ",
                    lines=15,
                    max_lines=30,
                    placeholder="ã“ã“ã«æ–‡å­—èµ·ã“ã—çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™...",
                    show_copy_button=True
                )
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
        gr.Markdown(f"""
        ### ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        - **ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹**: {device.upper()}
        - **ãƒ¢ãƒ‡ãƒ«**: nvidia/parakeet-tdt_ctc-0.6b-ja, silero-vad
        - **å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: WAV, MP3, FLAC, M4A ãªã©
        """)
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
        transcribe_btn.click(
            fn=process_audio_file,
            inputs=[audio_input],
            outputs=[output_text],
            show_progress=True
        )
    
    return interface

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒã‚ã‚‹å ´åˆã¯å¾“æ¥ã®å‡¦ç†ã‚’å®Ÿè¡Œ
    if len(sys.argv) > 1:
        try:
            asr, actual_device = load_model_with_fallback(device)
            print(f"[INFO] Model loaded successfully on {actual_device.upper()}")
            
        except Exception as e:
            print(f"[ERROR] Model loading failed: {e}")
            sys.exit(1)

        audio_files = sys.argv[1:]
        if not audio_files:
            print("[ERROR] No audio files specified")
            sys.exit(1)

        for audio_file in audio_files:
            print(f"[INFO] Processing: {audio_file}")
            try:
                # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªè»¢å†™ï¼ˆéŸ³å£°åˆ†å‰²å¯¾å¿œï¼‰
                transcriptions = transcribe_with_memory_management(asr, audio_file, actual_device)
                
                for text in transcriptions:
                    print(text)
                    
            except Exception as e:
                print(f"[ERROR] Transcription failed for {audio_file}: {e}")
            finally:
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                if actual_device == "cuda":
                    torch.cuda.empty_cache()
    else:
        # Gradio UIã‚’èµ·å‹•
        print("[INFO] Starting Gradio interface...")
        interface = create_gradio_interface()
        interface.launch(
            server_name="0.0.0.0",
            server_port=3791,
            share=False,
            show_error=True
        )